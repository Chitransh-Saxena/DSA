Use Case scenario - 

    When we create an application and try to run it locally, it runs on some specific port. (As usual)
    Now, what happens when we decide to put this application on some server like AWS, azure, heroku etc.
    So, now, there you have installed an OS (probably centOS, ubuntu or whatever), a DBMS (PG or Sybase), deployed the application.

Problem - 
    Now, when we ask customers to use this application then there is too much traffic on that port.

Possible Solution - 
    Can ask or automate a way to create new instances of the application for each set of users. (Based on DB limit)
    And that instance will be running on a different port.
    But the problem with this approach is that with time if the traffic even grows further, we may have to create another instance on another port.
    This is like an infinite problem. (For how long are we gonna create these ports and shifting users?)

    How does NGINX solve this problem?
        It creates a kind of gateway between the application and the port at which it will be running.
        User just has to start the application.
        Nginx will be taking all the requests from the user, and wherver this application is serving any customer, it will be talking to nginx.
        Nginx will be looking at all the servers and see which server or which instance has low amount of load, and it will be redirecting the requests to that port.

        But there is one more thing that nginx handles in a very sleek way.
            For each request, we know that application may be talking to DB and will be serving whatever request it is.
            But, for a case when we are releasing something new, or anything HOT and we know that a bulk of traffic would be coming up.
            Nginx loads up a simple cache. All the request for a particular route (a specific request in a application), so that application does not have to talk to DB repeatedly
            How does it do that?